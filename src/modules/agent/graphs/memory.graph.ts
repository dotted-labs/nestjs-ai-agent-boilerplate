// Main graph
import { AIMessage, BaseMessage } from '@langchain/core/messages';
import { END, LangGraphRunnableConfig, MemorySaver, START, StateGraph } from '@langchain/langgraph';
import { ConfigurationSchema, ensureConfiguration } from '../config/configuration';
import { GraphAnnotation } from '../config/state';
import { getStoreFromConfigOrThrow, loadChatModel } from '../config/utils';
import { initializeMemoryTool } from '../tools/memory.tool';

export const MEMORY_PROMPT = ` 
  Here is some information about the user:
  {user_info}
`;

// Define the function that calls the model
async function callModel(state: typeof GraphAnnotation.State, config: LangGraphRunnableConfig): Promise<{ messages: BaseMessage[] }> {
  const configuration = ensureConfiguration(config);
  const memories = await getMemories(config);

  const model = (await loadChatModel(configuration.model)).bindTools([initializeMemoryTool(config)]);
  const sys = configuration.systemPromptTemplate + MEMORY_PROMPT.replace('{user_info}', memories);
  const response = await model.invoke([{ role: 'system', content: sys }, ...state.messages]);

  return { messages: [response] };
}

// Define the function that stores new memories generated by the model
async function storeMemory(state: typeof GraphAnnotation.State, config: LangGraphRunnableConfig): Promise<{ messages: BaseMessage[] }> {
  const lastMessage = state.messages[state.messages.length - 1] as AIMessage;
  const toolCalls = lastMessage.tool_calls || [];

  const upsertMemoryTool = initializeMemoryTool(config);

  const savedMemories = await Promise.all(
    toolCalls.map(async (tc) => {
      return (await upsertMemoryTool.invoke(tc)) as BaseMessage;
    }),
  );

  return { messages: savedMemories };
}

// Define the function that fetches the most recent memories for the user
async function getMemories(config: LangGraphRunnableConfig): Promise<string> {
  const configuration = ensureConfiguration(config);
  const store = getStoreFromConfigOrThrow(config);
  const memories = await store.search(['memories', configuration.userId], { limit: 10 });

  let formatted = memories?.map((mem) => `[${mem.key}]: ${JSON.stringify(mem.value)}`)?.join('\n') || '';
  if (formatted) {
    formatted = `\n<memories>\n${formatted}\n</memories>`;
  }

  return formatted;
}

// Define the function that determines whether to continue or stop the graph based on model output
function routeModelOutput(state: typeof GraphAnnotation.State): 'store_memory' | typeof END {
  const lastMessage = state.messages[state.messages.length - 1] as AIMessage;
  // If the LLM has requested to upsert memories, route to the `store_memory` node.
  if (lastMessage.tool_calls?.length) {
    return 'store_memory';
  }
  // Otherwise end the graph.
  return END;
}

// Create the graph + all nodes
export const builder = new StateGraph(
  {
    stateSchema: GraphAnnotation,
  },
  ConfigurationSchema,
)
  .addNode('call_model', callModel)
  .addNode('store_memory', storeMemory)
  // Set the entrypoint as `call_model`
  // This means that this node is the first one called
  .addEdge(START, 'call_model')
  .addConditionalEdges(
    // First, we define the edges' source node. We use `call_model`.
    // This means these are the edges taken after the `call_model` node is called.
    'call_model',
    // Next, we pass in the function that will determine the sink node(s), which
    // will be called after the source node is called.
    routeModelOutput,
    {
      store_memory: 'store_memory',
      [END]: END,
    },
  )
  // This means that after `store_memory` is called, `call_model` node is called next.
  .addEdge('store_memory', 'call_model');

// Initialize a MemorySaver to persist checkpoints between calls
const memory = new MemorySaver();

// Finally, we compile it!
// This compiles it into a graph you can invoke and deploy.
export const graph = builder.compile({
  checkpointer: memory,
});
graph.name = 'MemoryAgent';
